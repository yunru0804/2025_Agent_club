{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langgraph transformers bitsandbytes langchain-huggingface langchain-community chromadb"
      ],
      "metadata": {
        "id": "IhCANVq5btqu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# 使用 4-bit 量化模型\n",
        "model_id = \"MediaTek-Research/Breeze-7B-Instruct-v1_0\"\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_threshold=6.0,\n",
        ")\n",
        "\n",
        "# 載入 tokenizer 與 4-bit 模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "921846e5c03146228c7aa2adbf03276f",
            "4abe873d0ceb4424bbf3e5a4d540e883",
            "f060bea83bdf438b841d9f5cf9de5579",
            "40ef126e38634db291240c2d953d6591",
            "508511128272485d81e0cada8e54bbcf",
            "a397b66d4b87488394730cabbd46943a",
            "342237584d154a38980e2367d6563bf3",
            "60a859d2de1e4021adeb8c7402ac83cf",
            "560a64c1965c4ada88ad12a0335cec88",
            "94685556ba28485c8934ddbc0c851c21",
            "5de6df3f6d404a06ad1cf7fba991352c"
          ]
        },
        "id": "bKoxjOTHb0Es",
        "outputId": "56826d6f-5b19-43c6-fd00-3f2c8791d749"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "921846e5c03146228c7aa2adbf03276f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立 text generation pipeline\n",
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")\n",
        "\n",
        "# 包裝成 LangChain 的 llm 物件\n",
        "llm = HuggingFacePipeline(pipeline=generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEInxV-Ub1Qt",
        "outputId": "c6dd8e22-166b-4d02-9f72-a51f03769caa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "docs_text = \"\"\"\n",
        "火影代數\t姓名\t師傅\t徒弟\n",
        "初代\t千手柱間\t無明確記載\t猿飛日斬、水戶門炎、轉寢小春\n",
        "二代\t千手扉間\t千手柱間（兄長）\t猿飛日斬、志村團藏、宇智波鏡等\n",
        "三代\t猿飛日斬\t千手柱間、千手扉間\t自來也、大蛇丸、千手綱手（傳說三忍）\n",
        "四代\t波風湊\t自來也\t旗木卡卡西、宇智波帶土、野原琳\n",
        "五代\t千手綱手\t猿飛日斬\t春野櫻、志乃等（主要為春野櫻）\n",
        "六代\t旗木卡卡西\t波風湊\t漩渦鳴人、宇智波佐助、春野櫻（第七班）\n",
        "七代\t漩渦鳴人\t自來也、旗木卡卡西\t木葉丸等（主要為木葉丸）\n",
        "\"\"\"\n",
        "\n",
        "docs = [Document(page_content=txt.strip()) for txt in docs_text.strip().split(\"\\n\\n\")]\n",
        "\n",
        "# chromadb 預設使用的大型語言模型為 \"all-MiniLM-L6-v2\"，由於該大型語言模型不支持中文，所以將模型替換為 \"infgrad/stella-base-zh-v3-1792d\"，並對 embedding 進行量化\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "persist_path_fd = \"document_store_fd\"\n",
        "collection_name_fd = \"naruto_collection_fd\"\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path_fd,\n",
        "    collection_name=collection_name_fd,\n",
        "    collection_metadata={\"hnsw:space\":\"cosine\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "womEwjsOb8MY",
        "outputId": "1e4e52d8-c4cd-4cd9-84aa-830140ab64c1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at infgrad/stella-base-zh-v3-1792d and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.4,\n",
        "    return_full_text=False # 僅返回生成的回應內容\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbpIx3Bab_Z4",
        "outputId": "b6e0eee7-40e6-460a-bcd3-51e3cbf034e6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class RAGState(TypedDict):\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "    history: List[str]"
      ],
      "metadata": {
        "id": "94xwzVqeb_xh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_path_ft = \"document_store_cosine_ft\"\n",
        "collection_name_ft = \"naruto_collection_cosine_ft\"\n",
        "vectorstore_text = Chroma.from_texts(\n",
        "    # documents=docs,\n",
        "    texts=[\"火影忍者的師承表\"],\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_path_ft,\n",
        "    collection_name=collection_name_ft,\n",
        "    collection_metadata={\"hnsw:space\":\"cosine\"}\n",
        ")"
      ],
      "metadata": {
        "id": "9-WnUnMncHGo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOPXxjexDMDZ"
      },
      "source": [
        "# baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hvn98KdEE9_"
      },
      "source": [
        "將`關鍵字`比對換成`向量相似度`比對。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4-rfIDDz7D"
      },
      "source": [
        "> 請將目前使用關鍵字比對的 route_by_query，改為使用向量相似度進行分類，並設一個合理的相似度門檻，根據檢索結果的分數判斷是否走 RAG 流程。  \n",
        "例如用向量相似度及自訂 threshold 決定要不要分到 retriever。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2jMeAOuJ2OM"
      },
      "source": [
        "> Hint：similarity_search_with_score(...)  \n",
        "可參考去年的讀書會 R4：向量資料庫的基本操作"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs = state[\"query\"], state[\"docs\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output}\n",
        "\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    prompt = f\"請回答以下問題：{query}\\n\\n回答：\"\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output}\n",
        "\n",
        "def route_by_query(state):\n",
        "  query = state[\"query\"]\n",
        "  docs = vectorstore_text.similarity_search_with_score(query=query,k=1)\n",
        "  distance = docs[0][1]  # type tuple\n",
        "  cosine_sim = 1 - distance\n",
        "  print(f\"route: cosine_sim = {cosine_sim}\")\n",
        "  choice = \"naruto\" if cosine_sim > 0.6 else \"general\"  #調整成0.7\n",
        "  # choice = \"naruto\" if any(word in query for word in keywords) else \"general\"\n",
        "  print(f\"跑到 → {choice}\")\n",
        "  return choice"
      ],
      "metadata": {
        "id": "3smmarB3cIaS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "2obhr9nndBiW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "誰是第四代火影?  \n",
        "第四代火影的師傅是誰?  \n",
        "第四代火影的徒弟有哪些人?  \n",
        "相對論是誰發明的?"
      ],
      "metadata": {
        "id": "Ic-kJht5cWnB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflAEri34xe5",
        "outputId": "db3b4716-8104-439d-e059-5514eb5e1aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 誰是第四代火影?\n",
            "route: cosine_sim = 0.6741631031036377\n",
            "跑到 → naruto\n",
            "回答： 第四代火影是波風湊。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的師傅是誰?  \n",
            "route: cosine_sim = 0.780278205871582\n",
            "跑到 → naruto\n",
            "回答： 第四代火影的師傅是自來也。\n",
            "============================================================ \n",
            "\n",
            "使用者: 第四代火影的徒弟有哪些人?\n",
            "route: cosine_sim = 0.7243319749832153\n",
            "跑到 → naruto\n",
            "回答： 第四代火影波風湊的徒弟有：旗木卡卡西、宇智波帶土、野原琳。\n",
            "============================================================ \n",
            "\n",
            "使用者: 相對論是誰發明的?\n",
            "route: cosine_sim = 0.32774388790130615\n",
            "跑到 → general\n",
            "回答： 相對論是由愛因斯坦（Albert Einstein）在1905年所發明的。他提出了「特異點」（special relativity）和「狹義相對論」（special relativity）等概念，重新定義了空間和時間的概念，並徹底改變了人類對宇宙的理解。\n",
            "============================================================ \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    init_state: RAGState = {\n",
        "        \"query\": user_input,\n",
        "        \"docs\": [],\n",
        "        \"answer\": \"\"\n",
        "    }\n",
        "\n",
        "    result = graph.invoke(init_state)\n",
        "    raw_output = result[\"answer\"]\n",
        "\n",
        "    answer_text = raw_output.split(\"回答：\")[-1].strip()\n",
        "    print(\"回答：\", answer_text)\n",
        "    print(\"===\" * 20, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8blDsDnDpbO"
      },
      "source": [
        "# advance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofkLZjzHENNT"
      },
      "source": [
        "改成能支援多輪問答（Multi-turn RAG），並能根據前面的query判斷問題。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2VXz7FxEONI"
      },
      "source": [
        "> 請將 RAGState 加入 history 欄位，並在生成回答時，將歷史對話與當前問題一起組成 prompt。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eSvoKFiKqP5"
      },
      "source": [
        "> Hint：\n",
        "```\n",
        "class MultiTurnRAGState(TypedDict):  \n",
        "    history: List[str]  \n",
        "    query: str  \n",
        "    docs: List[Document]  \n",
        "    answer: str\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, List\n",
        "\n",
        "# 定義 LangGraph 的 State 結構\n",
        "class MultiTurnRAGState(TypedDict):\n",
        "    query: str\n",
        "    docs: List[Document]\n",
        "    answer: str\n",
        "    history: List[str]"
      ],
      "metadata": {
        "id": "o9AknyHqflrM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_node(state: RAGState) -> RAGState:\n",
        "    query = state[\"query\"]\n",
        "    # similarity_search 距離越小越相似\n",
        "    docs = vectorstore.similarity_search(query, k=3)\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": \"\"}\n",
        "\n",
        "def generate_node(state: RAGState) -> RAGState:\n",
        "    query, docs, history = state[\"query\"], state[\"docs\"], state[\"history\"]\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "    prompt = (\n",
        "        f\"你是一個知識型助手，請根據以下內容回答問題：\\n\\n\"\n",
        "        f\"內容：{context}\\n\\n\"\n",
        "        f\"前情提要{history}\"\n",
        "        f\"問題：{query}\\n\\n回答：\"\n",
        "    )\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": docs, \"answer\": output, \"history\":f\"\"\"{history}|\\n\n",
        "用戶：{query}\n",
        "AI助理：{output}\"\"\"}\n",
        "\n",
        "def direct_generate_node(state: RAGState) -> RAGState:\n",
        "    query, history = state[\"query\"], state[\"history\"]\n",
        "    prompt = (f\"請回答以下問題：前情提要{history}{query}\\n\\n回答：\")\n",
        "    output = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return {\"query\": query, \"docs\": [], \"answer\": output, \"history\":f\"\"\"{history}|\\n\n",
        "用戶：{query}\n",
        "AI助理：{output}\"\"\"}\n",
        "\n",
        "def route_by_query(state):\n",
        "  query = state[\"query\"]\n",
        "  docs = vectorstore_text.similarity_search_with_score(query=query,k=1)\n",
        "  distance = docs[0][1]\n",
        "  cosine_sim = 1 - distance\n",
        "  print(f\"route: cosine_sim = {cosine_sim}\")\n",
        "  choice = \"naruto\" if cosine_sim > 0.5 else \"general\"  #調整成0.5\n",
        "  print(f\"跑到 → {choice}\")\n",
        "  return choice"
      ],
      "metadata": {
        "id": "T0_RFPtjfsgl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# 建立 LangGraph 流程圖\n",
        "graph_builder = StateGraph(RAGState)\n",
        "\n",
        "graph_builder.set_entry_point(\"condition\")\n",
        "graph_builder.add_node(\"condition\", RunnableLambda(lambda x: x))  # 進來就分流，不改變內容\n",
        "graph_builder.add_node(\"retriever\", RunnableLambda(retrieve_node))\n",
        "graph_builder.add_node(\"generator\", RunnableLambda(generate_node))\n",
        "graph_builder.add_node(\"direct_generator\", RunnableLambda(direct_generate_node))\n",
        "\n",
        "# 設定條件分流\n",
        "graph_builder.add_conditional_edges(\n",
        "    source=\"condition\",\n",
        "    path=RunnableLambda(route_by_query),\n",
        "    path_map={\n",
        "        \"naruto\": \"retriever\",\n",
        "        \"general\": \"direct_generator\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# 接下來的正常連接\n",
        "graph_builder.add_edge(\"retriever\", \"generator\")\n",
        "graph_builder.add_edge(\"generator\", END)\n",
        "graph_builder.add_edge(\"direct_generator\", END)\n",
        "\n",
        "# 編譯 Graph\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "UAaQuXygfuNz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第四代火影是誰?  \n",
        "他的師父是誰?  \n",
        "他的徒弟有哪些人?  \n",
        "相對論是他發明的嗎?  "
      ],
      "metadata": {
        "id": "TUCIN9q7gAFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Jrna_evTCC",
        "outputId": "c2fa6f9f-f9f2-4ee5-e34d-2b84901ff60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始對話吧（輸入 q 結束）\n",
            "使用者: 第四代火影是誰?\n",
            "route: cosine_sim = 0.658187747001648\n",
            "跑到 → naruto\n",
            "AI 助理: 第四代火影是波風湊。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的師父是誰?  \n",
            "route: cosine_sim = 0.5902388095855713\n",
            "跑到 → naruto\n",
            "AI 助理: 他的師父是自來也。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的徒弟有哪些人?  \n",
            "route: cosine_sim = 0.5774930715560913\n",
            "跑到 → naruto\n",
            "AI 助理: 波風湊的徒弟有旗木卡卡西、宇智波佐助、春野櫻（主要為春野櫻）。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: 他的徒弟有人有當火影嗎\n",
            "route: cosine_sim = 0.7625277638435364\n",
            "跑到 → naruto\n",
            "AI 助理: 旗木卡卡西和宇智波佐助都是有當火影的。旗木卡卡西是七代火影，宇智波佐助是六代火影。\n",
            "==================================================================================================================================================================================== \n",
            "\n",
            "使用者: q\n",
            "掰啦！\n"
          ]
        }
      ],
      "source": [
        "global_history: List[str] = []\n",
        "\n",
        "print(\"開始對話吧（輸入 q 結束）\")\n",
        "while True:\n",
        "    user_input = input(\"使用者: \")\n",
        "    if user_input.strip().lower() in [\"q\", \"quit\", \"exit\"]:\n",
        "        print(\"掰啦！\")\n",
        "        break\n",
        "\n",
        "    state = {\"history\": global_history, \"query\": user_input}\n",
        "    result = graph.invoke(state)\n",
        "\n",
        "    answer = result[\"answer\"].split(\"回答：\")[-1].strip()\n",
        "    print(\"AI 助理:\", answer)\n",
        "    print(\"===\" * 60, \"\\n\")\n",
        "\n",
        "    global_history = result[\"history\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(global_history)"
      ],
      "metadata": {
        "id": "trTmK2oDf8bK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "921846e5c03146228c7aa2adbf03276f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4abe873d0ceb4424bbf3e5a4d540e883",
              "IPY_MODEL_f060bea83bdf438b841d9f5cf9de5579",
              "IPY_MODEL_40ef126e38634db291240c2d953d6591"
            ],
            "layout": "IPY_MODEL_508511128272485d81e0cada8e54bbcf"
          }
        },
        "4abe873d0ceb4424bbf3e5a4d540e883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a397b66d4b87488394730cabbd46943a",
            "placeholder": "​",
            "style": "IPY_MODEL_342237584d154a38980e2367d6563bf3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f060bea83bdf438b841d9f5cf9de5579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a859d2de1e4021adeb8c7402ac83cf",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_560a64c1965c4ada88ad12a0335cec88",
            "value": 4
          }
        },
        "40ef126e38634db291240c2d953d6591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94685556ba28485c8934ddbc0c851c21",
            "placeholder": "​",
            "style": "IPY_MODEL_5de6df3f6d404a06ad1cf7fba991352c",
            "value": " 4/4 [01:20&lt;00:00, 16.40s/it]"
          }
        },
        "508511128272485d81e0cada8e54bbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a397b66d4b87488394730cabbd46943a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "342237584d154a38980e2367d6563bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60a859d2de1e4021adeb8c7402ac83cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560a64c1965c4ada88ad12a0335cec88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94685556ba28485c8934ddbc0c851c21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de6df3f6d404a06ad1cf7fba991352c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}